@article{sohn,
      title={FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence}, 
      author={Kihyuk Sohn and David Berthelot and Chun-Liang Li and Zizhao Zhang and Nicholas Carlini and Ekin D. Cubuk and Alex Kurakin and Han Zhang and Colin Raffel},
      year={2020},
      eprint={2001.07685},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.07685}, 
}
@article{tarvainen,
  author       = {Antti Tarvainen and
                  Harri Valpola},
  title        = {Weight-averaged consistency targets improve semi-supervised deep learning
                  results},
  journal      = {CoRR},
  volume       = {abs/1703.01780},
  year         = {2017},
  url          = {http://arxiv.org/abs/1703.01780},
  eprinttype    = {arXiv},
  eprint       = {1703.01780},
  timestamp    = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/TarvainenV17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{krizhevsky,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet classification with deep convolutional neural networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
journal = {Commun. ACM},
month = may,
pages = {84â€“90},
numpages = {7}
}
@article{simonyan,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.1556}, 
}
@article{he,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}
@article{selvaraju,
      title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
      author={Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
      year={2020},
      url={https://doi.org/10.1007/s11263-019-01228-7},
}
@INPROCEEDINGS{chattopadhay,
  author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks}, 
  year={2018},
  volume={},
  number={},
  pages={839-847},
  keywords={Visualization;Heating systems;Neurons;Machine learning;Predictive models;Mathematical model},
  doi={10.1109/WACV.2018.00097}
}
@article{wang,
      title={Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks}, 
      author={Haofan Wang and Zifan Wang and Mengnan Du and Fan Yang and Zijian Zhang and Sirui Ding and Piotr Mardziel and Xia Hu},
      year={2020},
      eprint={1910.01279},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1910.01279}, 
}
@article{long,
doi = {10.1088/1742-6596/1871/1/012071},
url = {https://dx.doi.org/10.1088/1742-6596/1871/1/012071},
year = {2021},
month = {apr},
publisher = {IOP Publishing},
volume = {1871},
number = {1},
pages = {012071},
author = {Jianwu Long and Zeran yan and Hongfa chen},
title = {A Graph Neural Network for superpixel image classification},
journal = {Journal of Physics: Conference Series},
abstract = {The classification of superpixel images by graph neural networks has gradually become a research hotspot. It is a crucial issue to embed super-pixel images from lowdimensional to high-dimensional so as to turn complex image information into graph signals. This paper proposes a method for image classification using a graph neural network (GNN) model. We convert the input image into a region adjacency graph (RAG) composed of superpixels as nodes, and use residual and concat structure to extract deep features. Finally, the loss function that increases the distance between classes and compactness within classes is used as supervision. Experiments have been tested with different numbers of superpixels on multiple datasets, and the results show that our method has a great performance in superpixel images classification.}
}
@misc{epflSLICSuperpixels,
	author = {EPFL},
	title = {{S}{L}{I}{C} {S}uperpixels --- epfl.ch},
	howpublished = {\url{https://www.epfl.ch/labs/ivrl/research/slic-superpixels/}},
	year = {},
	note = {[Accessed 08-12-2024]},
}